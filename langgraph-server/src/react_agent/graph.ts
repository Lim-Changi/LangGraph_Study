import { AIMessage, HumanMessage } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { StateGraph, Annotation } from "@langchain/langgraph";
import { ChromaClient } from "chromadb";

import { ConfigurationSchema, ensureConfiguration } from "./configuration.js";
import { TOOLS } from "./tools.js";
import { loadChatModel } from "./utils.js";

// Extended state annotation with routing information
const RouterStateAnnotation = Annotation.Root({
  messages: Annotation<any[]>,
  routingDecision: Annotation<"llm" | "vectordb" | "websearch" | null>,
  vectorContext: Annotation<string>,
  searchResults: Annotation<string>,
  finalResponse: Annotation<string>,
  isAccurate: Annotation<boolean>,
});

// ChromaDB client for vector search
const chromaClient = new ChromaClient({
  path: "http://localhost:8000",
});

// Simple embedding generation for vector search
function generateSimpleEmbedding(text: string): number[] {
  const embedding = new Array(384).fill(0);

  for (let i = 0; i < text.length && i < 384; i++) {
    const charCode = text.charCodeAt(i);
    embedding[i % 384] += charCode / 1000;
  }

  const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
  return embedding.map((val) => val / (norm || 1));
}

// Route message to appropriate handler based on LLM analysis
async function routeMessage(
  state: typeof RouterStateAnnotation.State,
  config: RunnableConfig
): Promise<typeof RouterStateAnnotation.Update> {
  const configuration = ensureConfiguration(config);
  const model = await loadChatModel(configuration.model);

  const lastMessage = state.messages[state.messages.length - 1];
  const messageContent =
    typeof lastMessage.content === "string"
      ? lastMessage.content
      : lastMessage.content.toString();

  // Use LLM to analyze the message and decide routing
  const routingPrompt = `ë‹¹ì‹ ì€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ë²•ì„ ê²°ì •í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë©”ì‹œì§€: "${messageContent}"

ë‹¤ìŒ ì˜µì…˜ ì¤‘ ê°€ì¥ ì ì ˆí•œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:

1. "llm": ì¼ë°˜ì ì¸ ì§ˆë¬¸, ëŒ€í™”, ì„¤ëª…, ì¶”ì²œ ë“±ì— ëŒ€í•œ ë‹µë³€
2. "vectordb": ë¬¸ì„œ, íŒŒì¼, ì—…ë¡œë“œëœ ìë£Œ, ì €ì¥ëœ ë°ì´í„°, íŠ¹ì • ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸
3. "websearch": ìµœì‹  ì •ë³´, ë‰´ìŠ¤, ì‹¤ì‹œê°„ ë°ì´í„°, ì›¹ì—ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸

JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"decision": "ì„ íƒí•œ_ì˜µì…˜", "reason": "ì„ íƒ ì´ìœ "}`;

  try {
    const response = await model.invoke([new HumanMessage(routingPrompt)]);

    const responseContent = response.content as string;
    const parsedResponse = JSON.parse(responseContent);

    const routingDecision = parsedResponse.decision as
      | "llm"
      | "vectordb"
      | "websearch";
    const reason = parsedResponse.reason || "No reason provided";

    console.log(`ğŸ”€ Message routed to: ${routingDecision}`);
    console.log(`ğŸ“ Reason: ${reason}`);
    console.log(`ğŸ“ Message content: ${messageContent.substring(0, 100)}...`);

    return { routingDecision };
  } catch (error) {
    console.error("Routing decision error:", error);
    console.log("ğŸ”„ Falling back to LLM routing");
    return { routingDecision: "llm" };
  }
}

// Handle LLM-based responses
async function handleLLMResponse(
  state: typeof RouterStateAnnotation.State,
  config: RunnableConfig
): Promise<typeof RouterStateAnnotation.Update> {
  const configuration = ensureConfiguration(config);
  const model = await loadChatModel(configuration.model);

  const response = await model.invoke([
    {
      role: "system",
      content: `You are a helpful AI assistant. Answer questions directly and concisely. Current time: ${new Date().toISOString()}`,
    },
    ...state.messages,
  ]);

  const finalResponse = response.content as string;
  console.log(`ğŸ¤– LLM Response: ${finalResponse.substring(0, 100)}...`);

  return { finalResponse, messages: [...state.messages, response] };
}

// Handle VectorDB search and response
async function handleVectorDBResponse(
  state: typeof RouterStateAnnotation.State,
  config: RunnableConfig
): Promise<typeof RouterStateAnnotation.Update> {
  try {
    const lastMessage = state.messages[state.messages.length - 1];
    const query =
      typeof lastMessage.content === "string"
        ? lastMessage.content
        : lastMessage.content.toString();

    console.log("ğŸ“š Attempting to search in vector database...");

    let vectorContext = "";

    try {
      // Get all collections
      const collections = await chromaClient.listCollections();
      console.log(
        `ğŸ“š Available collections: ${collections.map((c: any) => c.name).join(", ")}`
      );

      if (collections.length === 0) {
        console.log("ğŸ“š No collections found in ChromaDB");
        vectorContext = "ë¬¸ì„œ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.";
      } else {
        // Use the first collection for search
        const collectionName = (collections[0] as any).name;
        console.log(`ğŸ“š Searching in collection: ${collectionName}`);

        // Generate embedding for the query
        const queryEmbedding = generateSimpleEmbedding(query);

        // Try to get collection and search
        try {
          const collection = await chromaClient.getCollection({
            name: collectionName,
          } as any);

          // Search in the collection
          const searchResults = await collection.query({
            queryEmbeddings: [queryEmbedding],
            nResults: 5,
          });

          if (
            searchResults.documents &&
            searchResults.documents[0] &&
            searchResults.documents[0].length > 0
          ) {
            const documents = searchResults.documents[0];
            const metadatas = searchResults.metadatas?.[0] || [];
            const distances = searchResults.distances?.[0] || [];

            console.log(`ğŸ“š Found ${documents.length} relevant documents`);

            // Combine documents with metadata and distance information
            const contextParts = documents.map((doc, index) => {
              const metadata = metadatas[index] || {};
              const distance = distances[index] || 0;
              const source = metadata.source || "Unknown";
              const page = metadata.page || "";

              return `[ë¬¸ì„œ ${index + 1}] (ìœ ì‚¬ë„: ${(1 - distance).toFixed(3)}, ì¶œì²˜: ${source}${page ? `, í˜ì´ì§€: ${page}` : ""})
${doc}`;
            });

            vectorContext = contextParts.join("\n\n");
          } else {
            console.log("ğŸ“š No relevant documents found");
            vectorContext =
              "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.";
          }
        } catch (collectionError) {
          console.error("Collection access error:", collectionError);
          vectorContext =
            "ì»¬ë ‰ì…˜ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDB ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.";
        }
      }
    } catch (chromaError) {
      console.error("ChromaDB connection error:", chromaError);
      vectorContext =
        "ChromaDB ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. localhost:8000ì—ì„œ ChromaDBê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.";
    }

    console.log(
      `ğŸ“š Vector search completed for query: ${query.substring(0, 50)}...`
    );

    // Generate response using LLM with context
    const configuration = ensureConfiguration(config);
    const model = await loadChatModel(configuration.model);

    const contextPrompt =
      vectorContext &&
      !vectorContext.includes("ì‹¤íŒ¨") &&
      !vectorContext.includes("ì—†ìŠµë‹ˆë‹¤")
        ? `ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì°¸ê³  ë¬¸ì„œ:
${vectorContext}

ì‚¬ìš©ì ì§ˆë¬¸: ${query}

ë‹µë³€:`
        : `ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼: ${vectorContext}

ì‚¬ìš©ì ì§ˆë¬¸: ${query}

ìœ„ì˜ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.`;

    const response = await model.invoke([new HumanMessage(contextPrompt)]);

    const finalResponse = response.content as string;
    console.log(`ğŸ“š VectorDB Response: ${finalResponse.substring(0, 100)}...`);

    return {
      vectorContext,
      finalResponse,
      messages: [...state.messages, response],
    };
  } catch (error) {
    console.error("VectorDB search error:", error);
    const errorResponse =
      "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
    return {
      vectorContext: "",
      finalResponse: errorResponse,
      messages: [...state.messages, new AIMessage(errorResponse)],
    };
  }
}

// Handle web search and response
async function handleWebSearchResponse(
  state: typeof RouterStateAnnotation.State,
  config: RunnableConfig
): Promise<typeof RouterStateAnnotation.Update> {
  try {
    console.log(state);
    const lastMessage = state.messages[state.messages.length - 1];
    const query =
      typeof lastMessage.content === "string"
        ? lastMessage.content
        : lastMessage.content.toString();

    // Use Tavily search tool
    const searchTool = TOOLS.find((tool) => tool.name === "tavily_search");

    let searchResults = "";

    if (searchTool) {
      const searchResult = await searchTool.invoke({ query });
      searchResults =
        typeof searchResult === "string"
          ? searchResult
          : JSON.stringify(searchResult);
    }

    console.log(`ğŸ” Web search completed for: ${query}`);

    // Generate response using LLM with search results
    const configuration = ensureConfiguration(config);
    const model = await loadChatModel(configuration.model);

    const searchPrompt = searchResults
      ? `Based on the following web search results, please answer the user's question:

Search Results:
${searchResults}

Question: ${query}

Answer:`
      : `I couldn't perform a web search for: ${query}. Please try rephrasing your question.`;

    const response = await model.invoke([new HumanMessage(searchPrompt)]);

    const finalResponse = response.content as string;
    console.log(
      `ğŸ” Web Search Response: ${finalResponse.substring(0, 100)}...`
    );

    return {
      searchResults,
      finalResponse,
      messages: [...state.messages, response],
    };
  } catch (error) {
    console.error("Web search error:", error);
    const errorResponse = "ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
    return {
      searchResults: "",
      finalResponse: errorResponse,
      messages: [...state.messages, new AIMessage(errorResponse)],
    };
  }
}

// ì›¹ ê²€ìƒ‰ ê²°ê³¼ì˜ ì •í™•ì„± íŒë‹¨ ë…¸ë“œ
async function judgeWebSearchResult(
  state: typeof RouterStateAnnotation.State,
  config: RunnableConfig
): Promise<typeof RouterStateAnnotation.Update> {
  const configuration = ensureConfiguration(config);
  const model = await loadChatModel(configuration.model);

  const judgePrompt = `ì•„ë˜ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ì™€ ê·¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\nì´ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„íˆ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ \"yes\" ë˜ëŠ” \"no\"ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n\nì§ˆë¬¸: ${state.messages[0].content}\nì›¹ ê²€ìƒ‰ ê²°ê³¼: ${state.searchResults}\në‹µë³€: ${state.finalResponse}\n\nì •í™•í•©ë‹ˆê¹Œ? {\"result\": \"yes\" ë˜ëŠ” \"no\", \"reason\": \"ê°„ë‹¨í•œ ì´ìœ \"}`;

  try {
    const response = await model.invoke([new HumanMessage(judgePrompt)]);
    const parsed = JSON.parse(response.content as string);
    const isAccurate = parsed.result === "yes";
    return { isAccurate };
  } catch (e) {
    // ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ì •í™•í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨
    return { isAccurate: false };
  }
}

// ì •í™•ì„±ì— ë”°ë¼ ë¶„ê¸°í•˜ëŠ” í•¨ìˆ˜
function judgeWebSearchEdge(state: typeof RouterStateAnnotation.State): string {
  return state.isAccurate ? "__end__" : "router";
}

// Route to appropriate handler based on routing decision
function routeToHandler(state: typeof RouterStateAnnotation.State): string {
  const decision = state.routingDecision;

  switch (decision) {
    case "vectordb":
      return "vectordb_handler";
    case "websearch":
      return "websearch_handler";
    case "llm":
    default:
      return "llm_handler";
  }
}

// Define the new routing graph with multiple response handlers
const workflow = new StateGraph(RouterStateAnnotation, ConfigurationSchema)
  // Add routing node as entry point
  .addNode("router", routeMessage)

  // Add handler nodes for different response types
  .addNode("llm_handler", handleLLMResponse)
  .addNode("vectordb_handler", handleVectorDBResponse)
  .addNode("websearch_handler", handleWebSearchResponse)
  .addNode("judge_websearch_result", judgeWebSearchResult)

  // Set the entrypoint as router
  .addEdge("__start__", "router")

  // Add conditional edges from router to appropriate handlers
  .addConditionalEdges("router", routeToHandler)

  // All handlers lead to end, ë‹¨ websearchëŠ” judgeë¡œ
  .addEdge("llm_handler", "__end__")
  .addEdge("vectordb_handler", "__end__")
  .addEdge("websearch_handler", "judge_websearch_result")
  .addConditionalEdges("judge_websearch_result", judgeWebSearchEdge);

// Export the compiled graph
export const graph = workflow.compile({
  interruptBefore: [],
  interruptAfter: [],
});
